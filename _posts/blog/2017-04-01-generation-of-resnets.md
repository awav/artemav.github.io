---
layout: post
title: "Generation of Residuals"
categories: blog
date: 2017-04-01
---

Deep Learning has already transformed computing industry. Healthcare, security,
computer vision, speech and language analysis and many others topics has already
changed. Today, the buzzword _Deep Learning_ mostly associated with _Convolutional_
and _Recurrent_ neural networks. To be honest, there are huge amount of work around other
stuff such like Adversarial Neural Networks, Bayesian Neural Networks, Mixtures of Convolutions
and Recurrent nets. I was being torn to pieces by deciding what to write about first.
I decided to start from the culprip of Deep Learning success - the CNNs, by the way
this is not an American TV channel, and its derivatives residual networks
which hold all the aces.

Learning of deep neural networks is very time consuming. Even more powerful GPUs does not guarantee fast convergence
of neural networks.

It can be a distraction that very deep neural networks are much more effient than shallower, but taller.
In this context taller means


```yaml
markdown: kramdown
mathjax: true
```

Here is an example MathJax inline rendering \\( 1/x^{2} \\), and here is a block rendering:
\\[ \frac{1}{n^{2}} \\]

\\[ \sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6} \\]

The only thing to look out for is the escaping of the backslash when using markdown, so the delimiters become `\\[ ... \\]` and `\\( ... \\)` for inline and block maths respectively.
